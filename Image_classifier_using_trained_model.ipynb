{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sriramreddy/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#How to use the trained model\n",
    "#you have to create the same CNN which you have trained\n",
    "#Inorder to use the trained model, you have to create replicate the model which you have trained. \n",
    "#Image should be preprocessed in the way which it is fed during the training. \n",
    "#The following note book captures how well the model is behaving on different classification of the fruits. \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "# Import the backend\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fruit_images = []\n",
    "labels = [] \n",
    "for fruit_dir_path in glob.glob(\"/Users/sriramreddy/Downloads/523/ex2/train/*\"):\n",
    "    fruit_label = fruit_dir_path.split(\"/\")[-1]\n",
    "    for image_path in glob.glob(os.path.join(fruit_dir_path, \"*.png\")):\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        image = cv2.resize(image, (110, 110))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) \n",
    "        #why do we need to convert the RGB2BGR\n",
    "        #I don't think it is going to affect training\n",
    "        #BGR was a choice made for historical reasons and now we have to live with it. In other words, BGR is the horseâ€™s ass in OpenCV.\n",
    "        fruit_images.append(image)\n",
    "        labels.append(fruit_label)\n",
    "fruit_images = np.array(fruit_images)\n",
    "labels = np.array(labels)\n",
    "label_to_id_dict = {v:i for i,v in enumerate(np.unique(labels))}\n",
    "id_to_label_dict = {v: k for k, v in label_to_id_dict.items()}\n",
    "label_ids = np.array([label_to_id_dict[x] for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2698, 110, 110, 3), (2698,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_fruit_images = []\n",
    "validation_labels = [] \n",
    "for fruit_dir_path in glob.glob(\"/Users/sriramreddy/Downloads/523/ex2/test/*\"):\n",
    "    fruit_label = fruit_dir_path.split(\"/\")[-1]\n",
    "    for image_path in glob.glob(os.path.join(fruit_dir_path, \"*.png\")):\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        image = cv2.resize(image, (110, 110))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        validation_fruit_images.append(image)\n",
    "        validation_labels.append(fruit_label)\n",
    "validation_fruit_images = np.array(validation_fruit_images)\n",
    "validation_labels = np.array(validation_labels)\n",
    "validation_label_ids = np.array([label_to_id_dict[x] for x in validation_labels])\n",
    "validation_fruit_images.shape, validation_label_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sizes: (10901, 110, 110, 3) (2698, 110, 110, 3) (10901, 60) (2698, 60)\n",
      "Flattened: (10901, 36300) (2698, 36300)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = fruit_images, validation_fruit_images\n",
    "Y_train, Y_test = label_ids, validation_label_ids\n",
    "\n",
    "#Normalize color values to between 0 and 1\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "#Make a flattened version for some of our models\n",
    "X_flat_train = X_train.reshape(X_train.shape[0], 110*110*3)\n",
    "X_flat_test = X_test.reshape(X_test.shape[0], 110*110*3)\n",
    "\n",
    "#One Hot Encode the Output what is this 60 \n",
    "Y_train = keras.utils.to_categorical(Y_train, 60)\n",
    "Y_test = keras.utils.to_categorical(Y_test, 60)\n",
    "\n",
    "print('Original Sizes:', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "print('Flattened:', X_flat_train.shape, X_flat_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10901 samples, validate on 2698 samples\n",
      "Epoch 1/1\n",
      "10901/10901 [==============================] - 548s 50ms/step - loss: 1.7621 - acc: 0.4621 - val_loss: 0.8072 - val_acc: 0.6709\n",
      "Test loss: 0.8072312606361903\n",
      "Test accuracy: 0.6708673091178651\n"
     ]
    }
   ],
   "source": [
    "model_cnn = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model_cnn.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(110, 110, 3)))\n",
    "model_cnn.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model_cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_cnn.add(Dropout(0.25))\n",
    "model_cnn.add(Flatten())\n",
    "model_cnn.add(Dense(128, activation='relu'))\n",
    "model_cnn.add(Dropout(0.5))\n",
    "model_cnn.add(Dense(60, activation='softmax'))\n",
    "\n",
    "model_cnn.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_cnn.fit(X_train, Y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score = model_cnn.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I'm loading the weights\n",
    "fname=\"/Users/sriramreddy/Downloads/523/ex2/weights_cnn_classification_augmented_data.hdf5\"\n",
    "model_cnn.load_weights(fname)\n",
    "score = model_cnn.evaluate(X_test, Y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss after loading weights: 0.18243249262955427\n",
      "Test accuracy after loading weights: 0.949592290585619\n"
     ]
    }
   ],
   "source": [
    "print('Test loss after loading weights:', score[0])\n",
    "print('Test accuracy after loading weights:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 60) [[4.1474420e-01 2.8747948e-20 3.7118767e-12 5.8525580e-01 1.5474299e-22\n",
      "  7.1599966e-14 3.2037425e-18 8.9590591e-21 5.6318555e-22 1.2083604e-17\n",
      "  5.8864034e-19 1.2959383e-22 5.4487309e-18 3.0047478e-20 3.1845395e-18\n",
      "  2.2890178e-18 2.3104935e-18 5.8596469e-21 6.2133083e-23 1.2725730e-18\n",
      "  3.2080334e-19 4.2164237e-22 2.9573556e-17 1.7499023e-19 1.7644623e-18\n",
      "  3.1610383e-16 5.5884330e-22 1.5699200e-18 3.7210717e-19 1.4817105e-19\n",
      "  2.2796020e-22 1.7397926e-18 6.4252643e-20 8.0155621e-17 1.5872732e-20\n",
      "  1.4147785e-22 3.9447578e-20 8.3771277e-18 1.8371097e-19 3.1737080e-19\n",
      "  2.2650330e-20 5.4304321e-17 1.6882786e-19 9.7760827e-19 5.0910492e-20\n",
      "  1.5623847e-20 1.2897741e-21 4.7148546e-18 1.5278072e-18 7.4605190e-20\n",
      "  8.1016509e-21 3.2781519e-19 4.3955756e-21 2.9675345e-19 2.7854661e-19\n",
      "  4.2911510e-20 2.9735176e-19 9.0334294e-20 6.5976695e-18 8.6656276e-18]]\n"
     ]
    }
   ],
   "source": [
    "#Prediction of an image\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('/Users/sriramreddy/Downloads/523/ex2/load_test/apple.png', target_size = (110, 110))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "test_image = test_image/255\n",
    "result = model_cnn.predict(test_image)\n",
    "print(np.shape(result),result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([(0, 'freshapples'), (1, 'freshbanana'), (2, 'freshoranges'), (3, 'rottenapples'), (4, 'rottenbanana'), (5, 'rottenoranges')])\n"
     ]
    }
   ],
   "source": [
    "id_labels={0: 'freshapples',\n",
    " 1: 'freshbanana',\n",
    " 2: 'freshoranges',\n",
    " 3: 'rottenapples',\n",
    " 4: 'rottenbanana',\n",
    " 5: 'rottenoranges'}\n",
    "print(id_labels.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freshapples 0.830379746835443\n",
      "freshbanana 0.989501312335958\n",
      "freshoranges 0.9149484536082474\n",
      "rottenapples 0.9900166389351082\n",
      "rottenbanana 0.9943396226415094\n",
      "rottenoranges 0.9305210918114144\n",
      "Accuracy for all items 0.9477390659747962\n"
     ]
    }
   ],
   "source": [
    "accuracy={}\n",
    "infi,infi_set=0,0\n",
    "for k,v in id_labels.items():\n",
    "    total_set,count=0,0\n",
    "    for i in glob.glob(\"/Users/sriramreddy/Downloads/523/ex2/test/\"+v+\"/*png\"):\n",
    "        test_image = image.load_img(i, target_size = (110, 110))\n",
    "        test_image = image.img_to_array(test_image)\n",
    "        test_image = np.expand_dims(test_image, axis = 0)\n",
    "        test_image = test_image/255\n",
    "        result = model_cnn.predict(test_image)\n",
    "        total_set+=1\n",
    "        infi_set+=1\n",
    "        if result[0][k] >= 0.5:\n",
    "            count+=1\n",
    "            infi+=1\n",
    "    accuracy[k]=count/total_set\n",
    "    print(v,accuracy[k])\n",
    "print(\"Accuracy for all items\", infi/infi_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Augmented test data set\n",
    "    freshapples 0.5037974683544304\n",
    "    freshbanana 1.0\n",
    "    freshoranges 0.9072164948453608\n",
    "    rottenapples 0.6339434276206323\n",
    "    rottenbanana 0.36792452830188677\n",
    "    rottenoranges 0.47146401985111663\n",
    "Accuracy for all items 0.6293550778354337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freshapples 0.5517241379310345\n",
      "freshbanana 1.0\n",
      "freshoranges 0.9466019417475728\n",
      "rottenapples 0.6391437308868502\n",
      "rottenbanana 0.43790849673202614\n",
      "rottenoranges 0.5135135135135135\n",
      "Accuracy for all items 0.6604897418927862\n"
     ]
    }
   ],
   "source": [
    "accuracy={}\n",
    "infi,infi_set=0,0\n",
    "for k,v in id_labels.items():\n",
    "    total_set,count=0,0\n",
    "    for i in glob.glob(\"/Users/sriramreddy/Downloads/523/original_data_set/\"+v+\"/*png\"):\n",
    "        test_image = image.load_img(i, target_size = (110, 110))\n",
    "        test_image = image.img_to_array(test_image)\n",
    "        test_image = np.expand_dims(test_image, axis = 0)\n",
    "        result = model_cnn.predict(test_image)\n",
    "        total_set+=1\n",
    "        infi_set+=1\n",
    "        if result[0][k] >= 0.5:\n",
    "            count+=1\n",
    "            infi+=1\n",
    "    accuracy[k]=count/total_set\n",
    "    print(v,accuracy[k])\n",
    "print(\"Accuracy for all items\", infi/infi_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "The results for the original data set\n",
    "    freshapples 0.5431034482758621\n",
    "    freshbanana 1.0\n",
    "    freshoranges 0.9466019417475728\n",
    "    rottenapples 0.636085626911315\n",
    "    rottenbanana 0.434640522875817\n",
    "    rottenoranges 0.5045045045045045\n",
    "Accuracy for all items 0.6604897418927862"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freshapples 0.5478927203065134\n",
      "freshbanana 1.0\n",
      "freshoranges 0.9374325782092773\n",
      "rottenapples 0.6860346585117227\n",
      "rottenbanana 0.36782861292665214\n",
      "rottenoranges 0.474974974974975\n",
      "Accuracy for all items 0.6489447753511287\n"
     ]
    }
   ],
   "source": [
    "accuracy={}\n",
    "infi,infi_set=0,0\n",
    "for k,v in id_labels.items():\n",
    "    total_set,count=0,0\n",
    "    for i in glob.glob(\"/Users/sriramreddy/Downloads/523/augmented_data_set/\"+v+\"/*png\"):\n",
    "        test_image = image.load_img(i, target_size = (110, 110))\n",
    "        test_image = image.img_to_array(test_image)\n",
    "        test_image = np.expand_dims(test_image, axis = 0)\n",
    "        result = model_cnn.predict(test_image)\n",
    "        total_set+=1\n",
    "        infi_set+=1\n",
    "        if result[0][k] >= 0.5:\n",
    "            count+=1\n",
    "            infi+=1\n",
    "    accuracy[k]=count/total_set\n",
    "    print(v,accuracy[k])\n",
    "print(\"Accuracy for all items\", infi/infi_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
